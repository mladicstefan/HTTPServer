# This file strictly forbids all automated access

User-agent: *
Disallow: /

# Explicitly block common scrapers and crawlers
User-agent: Googlebot
Disallow: /

User-agent: Bingbot
Disallow: /

User-agent: Slurp
Disallow: /

User-agent: DuckDuckBot
Disallow: /

User-agent: Baiduspider
Disallow: /

User-agent: YandexBot
Disallow: /

User-agent: facebookexternalhit
Disallow: /

User-agent: Twitterbot
Disallow: /

User-agent: LinkedInBot
Disallow: /

# Block AI training crawlers
User-agent: GPTBot
Disallow: /

User-agent: ChatGPT-User
Disallow: /

User-agent: CCBot
Disallow: /

User-agent: anthropic-ai
Disallow: /

User-agent: Claude-Web
Disallow: /

User-agent: PerplexityBot
Disallow: /

User-agent: YouBot
Disallow: /

User-agent: Google-Extended
Disallow: /

# Block archive crawlers
User-agent: ia_archiver
Disallow: /

User-agent: Wayback
Disallow: /

User-agent: archive.org_bot
Disallow: /

# Block SEO and marketing crawlers
User-agent: AhrefsBot
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: DotBot
Disallow: /

User-agent: SeznamBot
Disallow: /

# Block content scrapers
User-agent: PaperLiBot
Disallow: /

User-agent: Applebot
Disallow: /

User-agent: WhatsApp
Disallow: /

User-agent: SkypeUriPreview
Disallow: /

# No sitemap provided
# Sitemap:

# Crawl delay for any bot that ignores disallow
Crawl-delay: 86400

# Request-rate: 1/86400
